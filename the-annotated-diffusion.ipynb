{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a3e276",
   "metadata": {},
   "source": [
    "# The Annotated Diffusion Model\n",
    "- https://huggingface.co/blog/annotated-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2978adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a8d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network helpers\n",
    "# Residual module: adds the input to the output of a particular function \n",
    "# (i.e., add residual connection to a particular function)\n",
    "# Also, alias for the up and down sampling operations\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        # Return the output of fn plus the input of function\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "    \n",
    "def upsample(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
    "\n",
    "def downsample(dim):\n",
    "    return nn.Conv2d(dim, dim, 4, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4c0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position embeddings\n",
    "# Sinusoidal position embeddings to encode t, inspired by Transformer\n",
    "# Input: Tensor (batch_size, 1) with noise levels of various noisy images in a batch\n",
    "# Output: Tensor (batch_size, dim) where dim = dimensionality of position embeddings\n",
    "# This is then added to each residual block\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] + embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f21a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet/ConvNext block\n",
    "# DDPM authors used Wide ResNet block\n",
    "# Phil Wang used ConvNeXT block though eventually removed from his implementation as\n",
    "# it didn't work well. Nonetheless, we'll continue using it as results were decent.\n",
    "# ConvNeXT = https://arxiv.org/abs/2201.03545\n",
    "# SiLU = Sigmoid Linear Unit aka \"swish\" (https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html)\n",
    "# GELU = Gaussian Error Linear Units (https://pytorch.org/docs/stable/generated/torch.nn.GELU.html)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = (scale + 1) * x + shift\n",
    "            \n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "        \n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.block1(x)\n",
    "        \n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            h = rearrange(time_emb, 'b c -> b c 1 1') + h\n",
    "            \n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "    \n",
    "\n",
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "        \n",
    "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
    "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, dim_out * mult),\n",
    "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.ds_conv(x)\n",
    "        \n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            condition = self.mlp(time_emb)\n",
    "            h = h + rearrange(condition, 'b c -> b c 1 1')\n",
    "            \n",
    "        h = self.net(h)\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07487af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention which is added in between the conv blocks. Two variants of attention\n",
    "# Regular multi-head self attention: As used in Transformer\n",
    "# Linear attention variant: Where the time and memory requirements scale linear in\n",
    "# sequence length as opposed to quadratic for regular attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv)\n",
    "        q = q * self.scale\n",
    "        \n",
    "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        \n",
    "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads), qkv)\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "        \n",
    "        q = q * self.scale\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "        \n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1e3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group normalization: DDPM interleaves convolution/attention layers of UNet with group\n",
    "# normalization. The PreNorm class here will be used to apply gorupnorm before the\n",
    "# attention layer\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e4df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional UNet (conditioned on the noise level)\n",
    "# Input: Batch of noisy images (batch_size, num_channels, height, width) and\n",
    "# noise level (batch_size, 1)\n",
    "# Output: Noise added to the output (batch_size, num_channels, height, width)\n",
    "\n",
    "# Network\n",
    "# Step 1: Convolutional layer is applied on the batch of noisy images, and position\n",
    "# embeddings are computed for the noise levels\n",
    "# Step 2: A sequence of downsampling stages are applied. Each downsampling stage has\n",
    "# two ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + downsample\n",
    "# Step 3: In the middle of network, ResNet/ConvNeXT blocks are applied, interleaved\n",
    "# with attention\n",
    "# Step 4: A sequence of upsampling stages are applied. Each upsampling stage has\n",
    "# two ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + upsample\n",
    "# Self 5: ResNet/ConvNeXT followed by a convolutional layer\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, dim, init_dim=None, out_dim=None, dim_mults=(1, 2, 4, 8),\n",
    "                 channels=3, with_time_emb=True, resnet_block_groups=8,\n",
    "                 use_convnext=True, convnext_mult=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensinos\n",
    "        self.channels = channels\n",
    "\n",
    "        init_dim = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        if use_convnext:\n",
    "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
    "        else:\n",
    "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        if with_time_emb:\n",
    "            time_dim = dim * 4\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim)\n",
    "            )\n",
    "        else:\n",
    "            time_dim = None\n",
    "            self.time_mlp = None\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        upsample(dim_in) if not is_last else nn.Identity()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        out_dim = default(out_dim, channels)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
    "\n",
    "        h = []\n",
    "\n",
    "        # downsample\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        # bottleneck\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        # upsample\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            # Combine the output of x and the hidden layer of last downsampling step\n",
    "            # See Unet image to understand better\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            x = upsample(x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93a4faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward diffusion process\n",
    "# Original DDPM used a linear schedule but Improving Denoising Diffusion Models showed\n",
    "# that better results can be achived with a cosine schedule\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps = 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x/timesteps) + s) / (1 + s) * torch.pi*0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda26c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with setting T = 200 and define the various variables from Beta_t, such as \n",
    "# the cumulative product of the variances (alpha_t). We also define an extract function\n",
    "# which will extract the appropriate t index for a batch of indices\n",
    "\n",
    "timesteps = 200\n",
    "\n",
    "# define beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# define alphas\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "# This is sqrt(1/alpha) which is used to downscale the clean image (to make diffusion tractable)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0/alphas)\n",
    "\n",
    "# calculations for diffusion q(x_t|x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# calculations for posterior q(x_{t-1} | x_t, x_O)\n",
    "# A sequence of posterior variances that is used to add noise during sampling\n",
    "posterior_variances = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da2b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 0   - Image weight: 0.9999, Noise weight: 0.0100\n",
      "timestep 1   - Image weight: 0.9998, Noise weight: 0.0173\n",
      "timestep 25  - Image weight: 0.9826, Noise weight: 0.1858\n",
      "timestep 50  - Image weight: 0.9357, Noise weight: 0.3527\n",
      "timestep 75  - Image weight: 0.8636, Noise weight: 0.5042\n",
      "timestep 100 - Image weight: 0.7723, Noise weight: 0.6353\n",
      "timestep 150 - Image weight: 0.5617, Noise weight: 0.8273\n",
      "timestep 199 - Image weight: 0.3636, Noise weight: 0.9316\n"
     ]
    }
   ],
   "source": [
    "# How does image weight and noise weight change during forward diffusion?\n",
    "# Image weight: Decreases from 0.999 to 0.3636\n",
    "# Noise weight: Increase from 0.010 to 0.932\n",
    "# Interesting that the sum of image and noise weights don't add to 1\n",
    "\n",
    "# Sample noisy images from clean image and timestep\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "    print(f'Image weight: {sqrt_alphas_cumprod_t}, Noise weight: {sqrt_one_minus_alphas_cumprod_t}')\n",
    "    \n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "for i in [0, 1, 25, 50, 75, 100, 150, 199]:\n",
    "    image_weight = extract(sqrt_alphas_cumprod, torch.tensor([i]), (1,))\n",
    "    noise_weight = extract(sqrt_one_minus_alphas_cumprod, torch.tensor([i]), (1, ))\n",
    "    print(f'timestep {str(i):3s} - Image weight: {image_weight.item():.4f}, Noise weight: {noise_weight.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5964d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 199 - Latent weight: 1.0102, Removed noise weight: 0.0215, Added noise weight: 0.1412\n",
      "timestep 150 - Latent weight: 1.0076, Removed noise weight: 0.0183, Added noise weight: 0.1224\n",
      "timestep 100 - Latent weight: 1.0051, Removed noise weight: 0.0159, Added noise weight: 0.0997\n",
      "timestep 75  - Latent weight: 1.0038, Removed noise weight: 0.0151, Added noise weight: 0.0862\n",
      "timestep 50  - Latent weight: 1.0026, Removed noise weight: 0.0145, Added noise weight: 0.0701\n",
      "timestep 25  - Latent weight: 1.0013, Removed noise weight: 0.0140, Added noise weight: 0.0491\n",
      "timestep 1   - Latent weight: 1.0001, Removed noise weight: 0.0115, Added noise weight: 0.0082\n",
      "timestep 0   - Latent weight: 1.0000, Removed noise weight: 0.0100, Added noise weight: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# How does latent and noise weight change during forward diffusion?\n",
    "# Latent weight: Stays about 1.00\n",
    "# Removed noise weight: Reduces from 0.0215 to 0.01\n",
    "# Added back noise: Reduces from 0.1412 to 0.00\n",
    "# Thus, we only remove very little noise (max 0.02, min 0.01) but add back in more noise (0.14, min 0)\n",
    "\n",
    "# Sample output images from noise and timestep\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Equation 11 in the paper: Use the model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t)/ sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    \n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variances, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        \n",
    "        # Algo 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "for i in reversed([0, 1, 25, 50, 75, 100, 150, 199]):\n",
    "    betas_t = extract(betas, torch.tensor([i]), (1,))\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, torch.tensor([i]), (1,))\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, torch.tensor([i]), (1,))\n",
    "    posterior_variance_t = extract(posterior_variances, torch.tensor([i]), (1,))\n",
    "    posterior_sd_t = torch.sqrt(posterior_variance_t)\n",
    "    removed_noise_weight = betas_t / sqrt_one_minus_alphas_cumprod_t\n",
    "    print(f'timestep {str(i):3s} - Latent weight: {sqrt_recip_alphas_t.item():.4f}, ' \\\n",
    "          f'Removed noise weight: {removed_noise_weight.item():.4f}, ' \\\n",
    "          f'Added noise weight: {posterior_sd_t.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca390959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample image\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d37b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to images via transformations\n",
    "# Step 1: Normalize images by dividing by 255 (so they are [0, 1] range)\n",
    "# Step 1: Make sure they are in [-1, 1] range\n",
    "image_size = 128\n",
    "transform = Compose([\n",
    "    Resize(image_size),\n",
    "    CenterCrop(image_size),\n",
    "    ToTensor(),  # Turn into numpy array of shape HWC and divice by 255 so in [0, 1] range\n",
    "    Lambda(lambda t: (t * 2) - 1)  # Set to [-1, 1] range\n",
    "])\n",
    "\n",
    "x_start = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02366522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca7c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse transform: Takes in Tensor with values [-1, 1] and return back to image\n",
    "reverse_transform = Compose([\n",
    "    Lambda(lambda t: (t + 1) / 2),\n",
    "    Lambda(lambda t: t.permute(1, 2, 0)),  # CHW to HWC\n",
    "    Lambda(lambda t: t * 255.),\n",
    "    Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "    ToPILImage()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812faf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_transform(x_start.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f72ca8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward diffusino process via the \"nice\" property\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "    print(f'Image weight: {sqrt_alphas_cumprod_t}, Noise weight: {sqrt_one_minus_alphas_cumprod_t}')\n",
    "    \n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "def get_noisy_image(x_start, t):\n",
    "    # Add noise\n",
    "    x_noise = q_sample(x_start, t=t)\n",
    "    \n",
    "    # Convert back to image\n",
    "    noisy_image = reverse_transform(x_noise.squeeze())\n",
    "    \n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c70046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([50])\n",
    "# get_noisy_image(x_start, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "042226e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adding noise across various timesteps\n",
    "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(figsize=(200, 200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [image] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090fdb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 25, 50, 75, 100, 199]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beff7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def p_losses(denoise_model, x_start, t, noise=None, loss_type='l1'):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "        \n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "    \n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'huber':\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noised)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset (we can use any dataset but here we use Fashion MNIST)\n",
    "dataset = load_dataset('fashion_mnist')\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d303240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations hich includes random horizonal flips which the paper \n",
    "# reported to improve sample quality slightly\n",
    "transform = Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 2)\n",
    "])\n",
    "\n",
    "def transforms(examples):\n",
    "    examples['pixel_values'] = [transform(image.convert('L')) for image in examples['image']]\n",
    "    del examples['image']\n",
    "    \n",
    "    return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms).remove_columns('label')\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset['train'], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034395fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model during training to trakc progress. How sampling is done:\n",
    "# Step 1: Sample pure noise from a Gaussian\n",
    "# Step 2: Use neural network to denoise it (via the learn conditional prob)\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Equation 11 in the paper: Use the model (noise predictor) to predict the mean\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t)/ sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    \n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variances, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        \n",
    "        # Algo 2 line 4:\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "    \n",
    "# Algorithm 2 (and returing all images)\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    b = shape[0]\n",
    "    \n",
    "    # Start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "    \n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "        \n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75946af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logic to periodically save generated images\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return rr\n",
    "\n",
    "results_folder = Path('./results')\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "save_and_sample_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac82eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and move to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca90bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = batch['pixel_values'].shape[0]\n",
    "        batch = batch['pixel_values'].to(device)\n",
    "        \n",
    "        # Algo 1 line 3: Sample t uniformly for each sample in the batch\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        losses = p_losses(model, batch, t, loss_type='huber')\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'Loss: {loss.item()}')\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # save generated images\n",
    "    if step != 0 and step % save_and_sample_every == 0:\n",
    "        milestone = step // save_and_sample_every\n",
    "        batches = num_to_groups(4, batch_size)\n",
    "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
    "        all_images = torch.cat(all_images_list, dim=0)\n",
    "        all_images = (all_images + 1) * 0.5\n",
    "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ebf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: sample 64 images\n",
    "samples = sample(model, image_size=image_size, batch_size=64, channels=channels)\n",
    "\n",
    "# show a random one\n",
    "random_index = 5\n",
    "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gif of denoising process\n",
    "random_index = 53\n",
    "\n",
    "fig = plt.figure()\n",
    "ims = []\n",
    "for i in range(timesteps):\n",
    "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
    "    ims.append([im])\n",
    "\n",
    "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "animate.save('diffusion.gif')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-to-image",
   "language": "python",
   "name": "text-to-image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
